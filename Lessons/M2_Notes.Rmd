---
Author: Maggie O'Shea
---

## Reviewing Basic Statistics Concepts 

**Variance**: Average of squared differences from mean
  - how far extremes are from your mean 

**Standard Deviation**: measure how spread out are the numbers 
  - distance from the mean 
  - 3rd standard deviations away from mean is the edge/outside of 99%
  - square of distance from value to the mean, sum distances and divide by number of observations and take square root 

## Stochastic Processes
  - Overall mean/SD is not a good representation of dataset because mean/SD is changing over time --> so if just look at mean then you will be off
  - How to address?
    - Mean *Function* 
    - Variance *function* 
    - Autocovariance *function*
    - Autocorrelation *function*
    - Each is a *function* of time 
    
  - **Mean Function** is //mu\\ sub t = E(Ysubt) --> expected value of process at time t 
  - **Variance Function** 
     - sd-t^2 = E(Yt^2) - mu-t^2
  - 

## Autocovariance Function
- *Autocovariance Function*: 
      - Cov(Yt1, Ytn) = E[YtYtn] - mutn*mut1
      
## Autocorrelation Function (ACF)
  - What is *correlation*? 
    - Covariance and correlation measure *joint variability* of two variables 
      - Join variability: how two variables are deviating from the mean (if you go above the mean on one, do you also go above the mean on the other? if yes, positive correlation)
  - What is *autocorrelation*?
    - It is a measure of dependence between two adjacent values of the same variables --> self correlation
        - Ex: correlation between temperature today and yesterday = autocorrelation
        - Best to rely on this for your model because you aren't introducing new uncertainty with projecting out explanatories variables
    - Lag 1 autocorrelation: correlation of today with yesterday -- looking one step back in the past, vs. Lag 2 which is correlation of t and t-2
  - *Autocorrelation Function*: Ro-t1-tn = Corr (Yt1, Ytn)
       - Corr(Yt1, Ytn) =  Cov(Yt1,Ytn)/sqrt(varianceYt1,varianceYtn)
       
  - How to compute autocorrelation --> Yt is the original series and Yn is the lagged version
      - Then can calculate the correlation between the two: the original and one lagged
      - Do as many lags as you can when testing autocorrelation 
          - you do remove the last values at the end when you keep lagging and so can't do it till the end
      - You want the autocorrelation to decay over time because you want the closest values together to be the most correlated which is when there is just a lag of 1

- **main(e) point** autocovariance and autocorrelation function give information about the dependence structure of a time series

    - Autocorrelation function close to +- 1 indicate strong linear dependence
    - Values of Autocorrelation functino close to 0 indicate weak linear dependence 
    - If autocorrelation function is exactly 0 then the two values are uncorrelated 

## Stationary Process

    - Basic idea of *stationarity* is that the probability laws that govern the behavior of the process do not change over time 
      - Stationary process when mean function is constant over time 
      - Distribution of observations at early time points is = to distribution of observations at later time points 
          - The mean should not change if you take the first set of points mean and the second set of points mean
    - Most are not stationary but you want to identify why not stationary 
    
  Consequences of Stationarity 
      - Distribution of Yt is the same of Yt-k for all t and all k 
          - Mean function will be constant, variance will be constant 
          - Autocovariance function will also be constant 
              - For example: the coviariance between Yt and Ys depends on the time difference between t and s and not on the actual times t and s 
              
White Noise Series: 
  - Example of stationary process: white noise series 
  - White noise series is a sequence of independent, identically distributed (i.i.d) random variables (et) 
  - {et} is a stationary process 
  - In time series modeling, assume white noise process has mean zero and constant variance 
      - Zero mean: because white noise is some variations over zero 
      - In time series modeling, Want residuals to be white noise (no autocorrelation in residual series, or a mean that is greater than zero)


## Partial Autocorrelation Function (PACF)
  - The ACF of a stationary process Yt at lag h measures the linear dependency among the process variables 
    - but the middle variables t-1, t-2, t-3, up to t-h influence this so have to remove the influence of all those intermediate variables to get the correlation between only Yt and Yt-h --> that is the partial autocorrelation function (just Yt and Yt-h, no intermediate variables)
    - the ACF and PACF measure the temporal dependency of a stochastic process 
    - The ACF and PACF give us information about the auto-regressive component of the series 
        - Auto-regressive: regressing on previous observations of Y 
    - The PACF for lag 1 and ACF for lag 1 will always be the same 
    - 
    
## PACF vs. ACF review

ACF: Yt = beta*Yt-1 + error term
      Yt = beta2*Yt-2 + error term 
      beta represents the ACF, the relationships between Yt and Yt-1, Yt and Yt-2

PACF: Yt = beta * Yt-1 + beta2*Yt-2 + error term
    This is accounting for the correlation for Yt and Yt-1 when looking at the correlation between Yt and Yt-2, so it essentially is just isolating *just* the relationship between Yt and Yt-2 by controlling for the intermediate variable's relationship --> the PACF value for Yt-2 is beta2 
    
    

