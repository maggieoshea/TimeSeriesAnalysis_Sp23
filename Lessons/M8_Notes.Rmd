---
Author: Maggie O'Shea
---

# M8: Model Diagnostics, Selection, Performance

## Videos 

  - Forecast Fit vs. Forecast Error
      - Forecast Fit: how well model fits observed values
      - Forecast Error: predictive power of selected model 
  - Forecast Fit: 
      - Backward-looking
      - describe difference between observed data and fitted values from model 
      - Residual Analysis: 
          - Describes the difference between actual historical data and fitted values generated by statistical model 
      - How well the model represents historical data
      - Help choose model that will be further used to forecast unobserved values 
          - forecast fit helps run model diagnostics and model selection 
  - Forecast Error
      - Done once you have the model 
      - Forward looking assessment
      - Difference between actual and forecasted values
      - Predictive power of the model 
      
### Model Selection 
  - With many predictors difficult to find best model 
  - How to select model?
      - Need criteria/benchmark to compare 2 models
      - Need a search strategy
  - Model Selection Criteria 
      - Some popular and well-known methods
          - AIC and AICc
              - Goodness of fit
          - BIC
          - F-Test
        - Bayseian Model Averaging
            - 2 models performing relatively well so can't choose between them
            - Instead use both models 
  - AIC
      - Estimator of quality of statistical models
      - Select model with lowest AIC
          - If all models are bad it will select the best of bad models, but still all are bad
      - Tradeoff between goodness of fit and the simplicity of the model 
      - AICc is used with the sample size (n) is small
  - BIC - Bayseian Information Criterion
      - Closely related to AIC, also estimator of quality of model 
      - Select with lowest BIC
      - Difference with AIC
          - BIC already incorporates sample size in value 
      - Sample Size should be much larger than N
      
### Residual Analysis 
  - Monitoring the Forecast
      - Tracking forecast errors and analyzing them can provide insight whether forecasts performing well
      - Sources of forecast errors:
          - Model may be inadequate
          - Irregular variations may have occurred
          - Forecasting technique incorrectly applied
          - Random variation 
              - Random variation is ok 
      - Residual Analyses useful for identifying presence of non-random error in forecasts 
  - Overview: 
      - Errors plotted on a chart in order that they occur
      - Forecasts are in control when: 
          - All errors within control limits
          - No Patterns are present 
  - Types of nonrandomness
      - error outside upper and lower control limits 
          - needs to be checked
      - Trend --> if residuals are in a line trending 
          - trend is not non-random, residuals are increasing/decreasing over time meaning that the trend was not modeled correctly
      - Cycling --> if residuals go up and down like seasonal 
          - Says that seasonal component not modeled correctly
      - Bias: too many points on one side of the center line
          - Mostly positive rather than negative which means that forecasts are always higher than observed values so are overestimating the desired variable 
  - Constructing a Control Chart
      - Compute the Mean Square Error
          - The square root of the MSE is used as estimate of standard deviation of distribution of errors
          - Errors are random therefore they will be distributed according to a normal distribution around a mean of zero 

### Model Performance
    - These criteria not measures of predictive power, just how well model fits observed data
    - From question of which models best explain observed, to which model best predicts
    - Model Performance (in contrast) measures forecast accuracy
    - Forecasters want to miminize forecast errors
        - impossible to perfectly forecast
        - Measure extent to which forecast might deviate from value of variable that actually occurs
    - Forecast accuracy should be important forecasting technique selection criteria 
        - Error = Actual (observed) - Forecast
        - If errors fall beyond acceptable bounds may need to correct
  - Common performance measures
      - Mean Error --> error = actual-forecast
      - Mean Squared Error --> don't care if error is positive or negative
      - Root Mean Squared Error (or Standard error) 
          - used to calculate standard deviation 
      - Coefficient of determination or R-squared (R2) 
          - how model explains variablility of variable
      - Mean Absolute Deviation or Mean Absolute Error 
          - Only considering absolute values
      - Mean Absolute Percent Error
          - Weights according to relative error
          - How much percentage deviated from value 
          - difficult to compare models from different datasets that have different magnitudes 
              - if difference is 3 forecast and actual value is 2 then you will get MAPE that says that's a horrible forecast, but if it's forecasted 90 and actual is 100 it will say it's great

        
        
        
  - Model Selection
      - Residual Analysis
      - AIC, AICc, and BIC
  - Performance Measures
      - MAD, MSE, MAPE
          - MAD percentage deviation from your forecast to actual value?
          - MSE - mean squared error --> want it to be as low as possible
          - Mean Absolute Percentage Error 
  - Forecast Fit
      - Backward Looking Assessment
      - Residual Analysis: descrbes the difference between actual historical data and fitted values generated by a statistical model 
  - Forecast Error
      - Forward-looking Assessment
      - Difference between actual and forecasted values 
  - AIC vs. BIC
      - AIC
          - Estimator of quality of statistical models
          - Select models with lowest AIC
          - Let k be the number of estimated parameters and L be the maximum value of likelihood function 
          
### Residual Analysis 
  - Want them to be idependent and identically distributed 
  - Mean squared error
      - MSE weights errors accroding to squared values
  - MAD
  - MAPE
      - MAPE weights errors according to relative error
      - difficult to compare models from different datasets that have different magnitudes 
          - if difference is 3 forecast and actual value is 2 then you will get MAPE that says that's a horrible forecast, but if it's forecasted 90 and actual is 100 it will say it's great

      
          
